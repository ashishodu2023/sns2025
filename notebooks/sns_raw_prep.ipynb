{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_traces\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbeam_settings_parser_hdf5\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeamConfigParserHDF5\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbeam_settings_prep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeamConfigPreProcessor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'"
     ]
    }
   ],
   "source": [
    "### Import Packages ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "#Jlab Packages\n",
    "from data_utils import get_traces\n",
    "from beam_settings_parser_hdf5 import BeamConfigParserHDF5\n",
    "from beam_settings_prep import BeamConfigPreProcessor\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPM Config ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config paths ###\n",
    "class BPMDataConfig:\n",
    "\n",
    "    def __ini__(self):\n",
    "        self.beam_settings_data_path = \"/work/data_science/suf_sns/beam_configurations_data/processed_data/clean_beam_config_processed_df.csv\"\n",
    "        self.beam_param_parser_cfg = {\"data_location\": \"/work/data_science/suf_sns/beam_configurations_data/hdf5_sept2024/\"}\n",
    "        self.beam_settings_prep_cfg = {\n",
    "    \"rescale\": False,\n",
    "    \"beam_config\": ['FE_IS:Match:TunerPos',\n",
    "                    'LEBT:Chop_N:V_Set',\n",
    "                    'LEBT:Chop_P:V_Set',\n",
    "                    'LEBT:Focus_1:V_Set',\n",
    "                    'LEBT:Focus_2:V_Set',\n",
    "                    'LEBT:Steer_A:V_Set',\n",
    "                    'LEBT:Steer_B:V_Set',\n",
    "                    'LEBT:Steer_C:V_Set',\n",
    "                    'LEBT:Steer_D:V_Set',\n",
    "                    'Src:Accel:V_Set',\n",
    "                    'Src:H2:Flw_Set',\n",
    "                    'Src:Ign:Pwr_Set',\n",
    "                    'Src:RF_Gnd:Pwr_Set',\n",
    "                    'ICS_Chop:RampDown:PW', # ICS_Chop-RampDown-PW\n",
    "                    'ICS_Chop:RampUp:PWChange', # ICS_Chop-RampUp-PWChange\n",
    "                    'ICS_MPS:Gate_Source:Offset', # ICS_MPS-Gate_Source-Offset\n",
    "                    'ICS_Tim:Chop_Flavor1:BeamOn', # ICS_Chop-BeamOn-Width\n",
    "                    'ICS_Tim:Chop_Flavor1:OnPulseWidth', # ICS_Chop-BeamOn-PW\n",
    "                    'ICS_Tim:Chop_Flavor1:RampUp', # ICS_Chop-RampUp-Width\n",
    "                    'ICS_Tim:Chop_Flavor1:StartPulseWidth', # ICS_Chop-RampUp-PW\n",
    "                    'ICS_Tim:Gate_BeamRef:GateWidth', # ICS_Tim-Gate_BeamRef-GateWidth\n",
    "                    'ICS_Tim:Gate_BeamOn:RR']} \n",
    "        self.column_to_add = [\n",
    "            'FE_IS:Match:TunerPos',\n",
    "            'LEBT:Chop_N:V_Set',\n",
    "            'LEBT:Chop_P:V_Set',\n",
    "            'LEBT:Focus_1:V_Set',\n",
    "            'LEBT:Focus_2:V_Set',\n",
    "            'LEBT:Steer_A:V_Set',\n",
    "            'LEBT:Steer_B:V_Set',\n",
    "            'LEBT:Steer_C:V_Set',\n",
    "            'LEBT:Steer_D:V_Set',\n",
    "            'Src:Accel:V_Set',\n",
    "            'Src:H2:Flw_Set',\n",
    "            'Src:Ign:Pwr_Set',\n",
    "            'Src:RF_Gnd:Pwr_Set',\n",
    "            'ICS_Tim:Gate_BeamOn:RR']\n",
    "\n",
    "    for entry in self.column_to_add:\n",
    "        configs[entry] = np.nan\n",
    "\n",
    "    self.configs = configs.rename(columns={\n",
    "        'ICS_Chop-RampDown-PW' : 'ICS_Chop:RampDown:PW', \n",
    "        'ICS_Chop-RampUp-PWChange' : 'ICS_Chop:RampUp:PWChange',\n",
    "        'ICS_MPS-Gate_Source-Offset' : 'ICS_MPS:Gate_Source:Offset',\n",
    "        'ICS_Chop-BeamOn-Width' : 'ICS_Tim:Chop_Flavor1:BeamOn',\n",
    "        'ICS_Chop-BeamOn-PW' : 'ICS_Tim:Chop_Flavor1:OnPulseWidth',\n",
    "        'ICS_Chop-RampUp-Width' : 'ICS_Tim:Chop_Flavor1:RampUp',\n",
    "        'ICS_Chop-RampUp-PW' : 'ICS_Tim:Chop_Flavor1:StartPulseWidth',\n",
    "        'ICS_Tim-Gate_BeamRef-GateWidth' : 'ICS_Tim:Gate_BeamRef:GateWidth',})  \n",
    "    \n",
    "    self.beam_config = [\n",
    "        'timestamps',\n",
    "        'FE_IS:Match:TunerPos',\n",
    "        'LEBT:Chop_N:V_Set',\n",
    "        'LEBT:Chop_P:V_Set',\n",
    "        'LEBT:Focus_1:V_Set',\n",
    "        'LEBT:Focus_2:V_Set',\n",
    "        'LEBT:Steer_A:V_Set',\n",
    "        'LEBT:Steer_B:V_Set',\n",
    "        'LEBT:Steer_C:V_Set',\n",
    "        'LEBT:Steer_D:V_Set',\n",
    "        'Src:Accel:V_Set',\n",
    "        'Src:H2:Flw_Set',\n",
    "        'Src:Ign:Pwr_Set',\n",
    "        'Src:RF_Gnd:Pwr_Set',\n",
    "        'ICS_Chop:RampDown:PW', # ICS_Chop-RampDown-PW\n",
    "        'ICS_Chop:RampUp:PWChange', # ICS_Chop-RampUp-PWChange\n",
    "        'ICS_MPS:Gate_Source:Offset', # ICS_MPS-Gate_Source-Offset\n",
    "        'ICS_Tim:Chop_Flavor1:BeamOn', # ICS_Chop-BeamOn-Width\n",
    "        'ICS_Tim:Chop_Flavor1:OnPulseWidth', # ICS_Chop-BeamOn-PW\n",
    "        'ICS_Tim:Chop_Flavor1:RampUp', # ICS_Chop-RampUp-Width\n",
    "        'ICS_Tim:Chop_Flavor1:StartPulseWidth', # ICS_Chop-RampUp-PW\n",
    "        'ICS_Tim:Gate_BeamRef:GateWidth', # ICS_Tim-Gate_BeamRef-GateWidth\n",
    "        'ICS_Tim:Gate_BeamOn:RR']\n",
    "\n",
    "\n",
    "\n",
    "    def configs_hist(self,dataframe, timestamp):\n",
    "        subset_columns = dataframe.columns.tolist()\n",
    "        subset_columns.remove(timestamp)\n",
    "        df_shifted = dataframe[subset_columns].shift(1)\n",
    "        mask = (dataframe[subset_columns] == df_shifted).all(axis=1)\n",
    "        dataframe = dataframe[~mask] #.reset_index(drop=True)\n",
    "        dataframe['time_diff'] = dataframe[timestamp].diff()\n",
    "        dataframe['timestamps_trm'] = dataframe[timestamp] + dataframe[timestamp].diff().shift(-1) - datetime.timedelta(seconds=0.000001)\n",
    "        subset_columns.insert(0, timestamp)\n",
    "        subset_columns.insert(1, \"timestamps_trm\")\n",
    "    \n",
    "        return dataframe[subset_columns]\n",
    "\n",
    "    def summary(self,text, df):\n",
    "        print(f'{text} shape: {df.shape}')\n",
    "        summ = pd.DataFrame(df.dtypes, columns=['dtypes'])\n",
    "        summ['null'] = df.isnull().sum()\n",
    "        summ['unique'] = df.nunique()\n",
    "        summ['min'] = df.min()\n",
    "        summ['median'] = df.median()\n",
    "        summ['max'] = df.max()\n",
    "        summ['mean'] = df.mean()\n",
    "        summ['std'] = df.std()\n",
    "        summ['duplicate'] = df.duplicated().sum()\n",
    "        return summ\n",
    "\n",
    "dc  = BPMDataConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FebMar22 .csv beam settings ###\n",
    "configs = pd.read_csv(dc.beam_settings_data_path)\n",
    "configs = configs.drop(\"Unnamed: 0\", axis=1, errors='ignore')\n",
    "configs['timestamps'] = pd.to_datetime(configs['timestamps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sep24 hdf5 beam settings ###\n",
    "parser = BeamConfigParserHDF5(dc.beam_param_parser_cfg)\n",
    "data, _ = parser.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Prepared datasets ###\n",
    "prep = BeamConfigPreProcessor(dc.beam_settings_prep_cfg)\n",
    "prepared_settings, run_cfg = prep.run(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = configs.loc[:, beam_config]\n",
    "bpm = pd.concat([configs, prepared_settings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.summary('bpm_summary', bpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCM Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCMDatConfig:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset1_loc = \"/work/data_science/suf_sns/DCM_Errant/\"\n",
    "        self.dataset2_loc = \"/w/data_science-sciwork24/suf_sns/DCML_dataset_Sept2024\"\n",
    "        self.start_date = 20220218\n",
    "        self.end_date = 20220318\n",
    "        self.anomaly_type = \"00110000\" #--  48\n",
    "        self.length_of_waveform = 10000\n",
    "        self.exclude_dates = [20220220, 20220221, 20220222, 20220223, 20220301, 20220308, 20220309, 2022015]\n",
    "        self.filtered_normal_files = []\n",
    "        self.filtered_anomaly_files = []\n",
    "        self.filtered_normal_files2 = []\n",
    "        self.filtered_anomaly_files2 = []\n",
    "\n",
    "\n",
    "    def GetFebFilteredFiles(self):\n",
    "        for root, subfolders, files in os.walk(self.dataset1_loc):\n",
    "            for file in files:\n",
    "                if '.bin' in file and 'DCML' in file:\n",
    "                    try:\n",
    "                        date = int(file[:8])\n",
    "                    except:\n",
    "                        print(\"Error in filename: \", file)\n",
    "                        print(\"-- Date could not be read!\")\n",
    "                        continue\n",
    "                if date >= self.start_date and date <= self.end_date:\n",
    "\n",
    "                if date in self.exclude_dates:\n",
    "                    continue\n",
    "\n",
    "                if '00000000' in file:\n",
    "                    self.filtered_normal_files.append(os.path.join(root, file))\n",
    "                elif self.anomaly_type in file:\n",
    "                    self.filtered_anomaly_files.append(os.path.join(root, file))\n",
    "\n",
    "        print('Number of available normal files: ', len(self.filtered_normal_files))\n",
    "        print('Number of available anomaly files: ', len(self.filtered_anomaly_files))\n",
    "        return self.filtered_normal_files,self.filtered_anomaly_files\n",
    "\n",
    "    def GetSepFilteredFiles(self):\n",
    "        subfolders = [ f.path for f in os.scandir(self.dataset2_loc) if f.is_dir() ]\n",
    "        for directory in subfolders:\n",
    "            if \"normal\" in directory or \"anomal\" in directory:                \n",
    "                for root, subfolders, files in os.walk(directory):\n",
    "                    for file in files:\n",
    "                        full_path = root\n",
    "                            if \".gz\" in file:\n",
    "                                if 'normal' in directory:\n",
    "                                    self.filtered_normal_files2.append(os.path.join(full_path, file))\n",
    "                                elif \"anomal\" in directory:\n",
    "                                    self.filtered_anomaly_files2.append(os.path.join(full_path, file))\n",
    "                        \n",
    "        print('Number of available normal files: ', len(self.filtered_normal_files2))\n",
    "        print('Number of available anomaly files: ', len(self.filtered_anomaly_files2))\n",
    "        return self.filtered_normal_files2,self.filtered_anomaly_files2\n",
    "\n",
    "\n",
    "\n",
    "dcm = DCMDatConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Feb Data Details ###\n",
    "filtered_normal_files,filtered_anomaly_files=dcm.GetFebFilteredFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Sep Data Details ###\n",
    "filtered_normal_files2,filtered_anomaly_files2==dcm.GetSepFilteredFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Traces and Timestamps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTracesAndTs(filtered_files):\n",
    "    index = np.random.randint(0, len(filtered_files))\n",
    "    filepath = filtered_files[index]\n",
    "    print(index)\n",
    "    try:\n",
    "        traces, timestamps = get_traces(filepath, var_id=\"Trace2\", begin=3000, shift=length_of_waveform, data_type=0)\n",
    "    except Exception as e:\n",
    "        traces = []\n",
    "        timestamps = []\n",
    "        print(\"Error in reading the file: \", filepath)\n",
    "        print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetTracesAndTs(filtered_normal_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeDatasets:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "        self.timestamps = []\n",
    "        self.flag = []\n",
    "        self.file = []\n",
    "\n",
    "    def IterateFiles(self,file_paths,MonthName):\n",
    "        for dcml in file_paths[:10]:\n",
    "        tmp_trace, tmp_timestamp = get_traces(dcml, var_id=\"Trace2\", begin=3000, shift=length_of_waveform, data_type=0)\n",
    "        tmp_trace = np.array(tmp_trace[1:])\n",
    "        tmp_timestamp = np.array(tmp_timestamp[1:])\n",
    "            for sample in tmp_trace:\n",
    "                traces.append(sample)\n",
    "                flag.append(0)\n",
    "                file.append(MonthName)\n",
    "            for time in tmp_timestamp:\n",
    "                timestamps.append(time)\n",
    "\n",
    "    def MergeDatasets(self):\n",
    "        merged_df = pd.merge_asof(dcm.sort_values(\"timestamps\"), bpm.sort_values(\"timestamps\"), on=\"timestamps\", direction=\"nearest\")\n",
    "        return merged_df\n",
    "\n",
    "\n",
    "md = MergeDatasets()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = md.MergeDatasets()\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing ##\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "      \n",
    "        self.df = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "\n",
    "    def check_nan(self):\n",
    "       \n",
    "        nan_counts = self.df.isna().sum()\n",
    "        return nan_counts[nan_counts > 0]\n",
    "\n",
    "    def remove_nan(self):\n",
    "      \n",
    "        self.df.dropna(inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def check_null(self):\n",
    "        \n",
    "        null_counts = self.df.isnull().sum()\n",
    "        return null_counts[null_counts > 0]\n",
    "\n",
    "    def remove_null(self):\n",
    "        \n",
    "        self.df.dropna(inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def check_duplicates(self):\n",
    "        \"\n",
    "        return self.df.duplicated().sum()\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def check_outliers(self):\n",
    "        \n",
    "        outlier_dict = {}\n",
    "        for col in self.df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = self.df[(self.df[col] < (Q1 - 1.5 * IQR)) | (self.df[col] > (Q3 + 1.5 * IQR))].shape[0]\n",
    "            if outliers > 0:\n",
    "                outlier_dict[col] = outliers\n",
    "        return outlier_dict\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        \n",
    "        for col in self.df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = self.df[col].quantile(0.25)\n",
    "            Q3 = self.df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            self.df = self.df[(self.df[col] >= lower_bound) & (self.df[col] <= upper_bound)]\n",
    "        return self.df\n",
    "\n",
    "    def convert_float64_to_float32(self):\n",
    "       \n",
    "        float64_cols = self.df.select_dtypes(include=['float64']).columns\n",
    "        self.df[float64_cols] = self.df[float64_cols].astype('float32')\n",
    "        return self.df.dtypes\n",
    "\n",
    "    def rename_columns(self, rename_dict):\n",
    "       \n",
    "        self.df.rename(columns=rename_dict, inplace=True)\n",
    "        return self.df.head()\n",
    "\n",
    "    def get_dataframe(self):\n",
    "       \n",
    "        return self.df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
